{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T10 Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate an artifical dataset consisting of three variables. The three variables show some dynamics as function of time which varies from [0,10] s. \n",
    "\n",
    "We will perform a Principal Component Analysis on this dataset. The aim is to find two latent varaibles which could account for the dynamics of the original three variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries and creating the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# generating three time-dependent variables \n",
    "t = np.arange(1000)/100. # time array \n",
    "x1 = np.sin(t) + t/1. + np.random.rand(len(t))/2.\n",
    "x2 = np.sin(t) - t/1. + np.random.rand(len(t))/2.\n",
    "x3 = t/1. + np.random.rand(len(t))/2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the raw data\n",
    "\n",
    "As usual let's start by plotting the data first. Simply display the three variables as function of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the raw data in 3D\n",
    "\n",
    "Let's also look at the three variables in the 3-dimensional space, where x1, x2 and x3 are used as x, y and z. You can use the matplotlib function `plot3D()` which takes `x, y` and `z` as input arguments. Note that time is not explictely plotted in this depiction. It is implicit in the trajectory of the data. \n",
    "\n",
    "What do you observe when rotating the axes (use the interactive modus through `%matplotlib notebook`)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the correlation structure between variables\n",
    "\n",
    "PCA removes correlations between variables. Let's see whether correlations between the three variables exist. In other words, let's calculate the covariance matrix. Use the matplotlib `np.cov()` function with the three variables as input argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the PCA \n",
    "\n",
    "Let's now move on to perform the PCA analysis. We will use the scikit learn module, specifically the `PCA` function in `sklearn.decomposition`. We will look at the PCA components (via `pca.components_`, the explained variance (via `pca.explained_variance_`) and the covariance matrix from the PCA (via `pca.get_covariance()`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform PCA analysis \n",
    "X = np.column_stack((x1,x2,x3)) # first we have to concatenate the three variables \n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize principal compnents in 3D\n",
    "\n",
    "Let's plot the first two basis vectors in the 3-dimensional space. What is the spatial relation between the three variables and the Eigenvectors? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = pca.components_\n",
    "\n",
    "\n",
    "#ax.plot([0,3*comp[0,0]],[0,3*comp[0,1]],[0,3*comp[0,2]])\n",
    "#ax.plot([0,3*comp[1,0]],[0,3*comp[1,1]],[0,3*comp[1,2]])\n",
    "#### Visualize the raw data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the original data in the PCA space\n",
    "\n",
    "Next, let's now convert the data to the new space spanned by the principal components by using `pca.transform([original data])`. Plot the two first latent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform the variables into the prinipal component space \n",
    "\n",
    "# plot new variables in the PCA space \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform inverse tranformation\n",
    "\n",
    "Finally, let's look at the inverse tranformation, i.e., the transformation from the PCA space back to the original coordinate system. Perform the inverse transformation for the three variables. Plot the inversely transformed variables in the orginal space and compare with the original variables. \n",
    "\n",
    "*Hint :* The function `pca.inverse_transform()` allows to perform the inverse transformation and takes the variables in the pca space as input arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse transformation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the inverse tranformation without taking into accout the first principal component. What does this mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstract original data without 1st principal component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
